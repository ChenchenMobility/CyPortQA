{"cells":[{"cell_type":"code","execution_count":null,"id":"8a1034ad","metadata":{"id":"8a1034ad"},"outputs":[],"source":["from langchain_community.chat_models import ChatOllama\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.messages import HumanMessage\n","from langchain_core.messages import SystemMessage\n","import os\n","import time\n","import pandas as pd\n","import base64\n","from joblib import Parallel, delayed\n","from pathlib import Path\n","import random\n"]},{"cell_type":"code","execution_count":null,"id":"079eb594","metadata":{"id":"079eb594","outputId":"820d84a2-885a-47a7-d787-798b62dcb38e"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\chenh\\AppData\\Local\\Temp\\ipykernel_54080\\3916901129.py:11: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n","  llm = ChatOllama(model=LLMName)\n"]}],"source":["InputFileFolder = 'MultiModalInputs'\n","LLMName = 'mistral-small3.1:24b'\n","\n","# already installed LLM that supports multimodal inputs\n","#llava:latest\n","#llama4:16x17b\n","#gemma3:12b\n","#qwen2.5vl:7b\n","#mistral-small3.1:24b\n","#bakllava:7b\n","llm = ChatOllama(model=LLMName)\n","llm.invoke('Hello, please show me your LLM model version.')\n","\n","# GROK\n","\n","# create an output folder\n","safe_LLMName = LLMName.replace(':', '_')\n","path = Path(f\"MultiModalOutputs/{safe_LLMName}\")\n","path.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"id":"3a79818c","metadata":{"id":"3a79818c"},"outputs":[],"source":["import json\n","with open(\"Final_QA_JSON_filtered_0723.json\", \"r\", encoding=\"utf-8\") as f:\n","    QASet = json.load(f)\n","EventNames = list(QASet.keys())"]},{"cell_type":"code","execution_count":null,"id":"469e5026","metadata":{"id":"469e5026"},"outputs":[],"source":["def write_message(Q_current):\n","    # write a message given the current question format. This message is submission-ready to LLM\n","    # Q_current = QASet[EventNames[23]]['qa'][123]\n","\n","    Q_modalities = Q_current['modalities']\n","    storm, year, leadtime = Q_current['context'][-3:]\n","\n","    human_content = []\n","    for modality in Q_modalities:\n","        filename = f\"{storm}_{year}_{leadtime}h\"\n","        folder_path = modality\n","\n","        if (modality == 'Graphic_Uncertainty_cone') or (modality == 'Graphic_Wind'):\n","            image_path = os.path.join(InputFileFolder, folder_path, f\"{filename}.PNG\")\n","            with open(image_path, \"rb\") as img_file:\n","                image_b64 = base64.b64encode(img_file.read()).decode(\"utf-8\")\n","                human_content.append({\n","                    \"type\": \"image_url\",\n","                    \"image_url\": {\"url\": f\"data:image/gif;base64,{image_b64}\"}\n","                })\n","\n","        elif (modality == 'text_advisory') or (modality == 'Table_wind'):\n","            text_path = os.path.join(InputFileFolder, folder_path, f\"{filename}.txt\")\n","            with open(text_path, \"r\", encoding=\"utf-8\") as text_file:\n","                advisory_text = text_file.read()\n","                human_content.append({\n","                    \"type\": \"text\",\n","                    \"text\": advisory_text\n","                })\n","        else:\n","            raise ValueError('Unknown Modality Input!')\n","\n","    # Append question last\n","    human_content.append({\n","        \"type\": \"text\",\n","        \"text\": Q_current['question']\n","    })\n","\n","    # Assemble messages\n","    messages = [\n","        SystemMessage(content=Q_current['prompt']),\n","        HumanMessage(content=human_content)\n","    ]\n","\n","    true_answer = Q_current['answer']\n","\n","    return(messages,true_answer)"]},{"cell_type":"markdown","id":"ff6477cd","metadata":{"id":"ff6477cd"},"source":["Loop over each event\n"]},{"cell_type":"code","execution_count":null,"id":"33fd2232","metadata":{"scrolled":true,"id":"33fd2232"},"outputs":[],"source":["def run_LLM(EventIdx):\n","\n","    EventName_thisevent = EventNames[EventIdx]\n","    print(f\"EventIdx = {EventIdx}, EventName = {EventName_thisevent}ï¼Œ LLM = {LLMName}\")\n","    QASet_thisevent = QASet[EventNames[EventIdx]]['qa']\n","    QASet_thisevent_indexed = list(enumerate(QASet_thisevent))\n","    random.shuffle(QASet_thisevent_indexed)  # Shuffle\n","\n","    results_dict = {}\n","\n","    for idx_q,q in enumerate(QASet_thisevent_indexed):\n","        t1 = time.time()\n","        message_thisq,true_answer = write_message(q)\n","        response = llm.invoke(message_thisq)\n","        t2 = time.time()\n","\n","        # Store result\n","        results_dict[idx_q] = {\n","            \"response\": response.content.strip(),\n","            \"ground_truth\": true_answer\n","        }\n","\n","        # Step 5: Sort results by original index and convert to list\n","        results_thisevent = [results_dict[idx] for idx in sorted(results_dict)]\n","\n","\n","        print(f\"Evtid = {EventIdx}, Q_idx = {idx_q},  elasped time = {t2-t1:.4f}\")\n","\n","    df = pd.DataFrame(results_thisevent)\n","    df.to_csv(os.path.join(f\"MultiModalOutputs/{safe_LLMName}/EvtID{EventIdx}_{EventName_thisevent}.csv\"), index=False)"]},{"cell_type":"code","execution_count":null,"id":"81cb48bb","metadata":{"scrolled":true,"id":"81cb48bb","outputId":"1e2c8b25-dbbf-4635-f9d1-2ef8747fd0b2"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mloky\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_LLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevntidx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevntidx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mEventNames\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\LLM\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n","\u001b[31mKeyboardInterrupt\u001b[39m: "]}],"source":["Parallel(n_jobs = -1,backend=\"loky\")(\n","        delayed(run_LLM)(evntidx)\n","        for evntidx in range(len(EventNames))\n","    )"]},{"cell_type":"code","execution_count":null,"id":"c7e60a49","metadata":{"id":"c7e60a49"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"bf7f568b","metadata":{"id":"bf7f568b"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}