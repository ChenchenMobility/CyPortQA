{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecf0d57e-bc86-469e-ab53-ae83e2513e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"May 10, 2015 - 5 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"June 16, 2015 - 4 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"July 12, 2015 - 8 PM PDT\" included an un-recognized timezone \"PDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"July 14, 2015 - 11 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"November 9, 2015 - 1 PM EST\" included an un-recognized timezone \"EST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"May 29, 2016 - 11 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"June 6, 2016 - 11 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 2, 2016 - 2 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 13, 2016 - 11 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 20, 2016 - 8 PM PDT \" included an un-recognized timezone \"PDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 8, 2016 - 11 AM EDT \" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"June 22, 2017 - 4 AM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"July 31, 2017 - 2 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 15, 2017 - 5 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 25, 2017 - 10 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 10, 2017 - 2 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 1, 2017 - 9 AM MDT\" included an un-recognized timezone \"MDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 9, 2017 - 11 AM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 8, 2017 - 10 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 18, 2017 - 11 PM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 5, 2017 - 8 AM EDT \" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 28, 2017 - 5 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"July 8, 2018 - 8 PM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"July 8, 2018 - 5 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 14, 2018 - 5 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 5, 2018 - 1 AM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 2, 2018 - 3 AM MDT\" included an un-recognized timezone \"MDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 10, 2018 - 1 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"July 13, 2019 - 10 AM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"July 23, 2019 - 5 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 7, 2019 - 8 PM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 28, 2019 - 5 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 4, 2019 - 1 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 14, 2019 - 11 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 25, 2019 - 5 PM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 17, 2019 - 4 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 24, 2019 - 5 PM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 11, 2019 - 11 AM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 19, 2019 - 1 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 25, 2019 - 10 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"November 19, 2019 - 11 AM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"May 18, 2020 - 11 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"May 27, 2020 - 11 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"June 3, 2020 - 1 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"July 10, 2020 - 5 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"July 25, 2020 - 7 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 3, 2020 - 8 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 16, 2020 - 5 AM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 14, 2020 - 5 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 27, 2020 - 1 AM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 24, 2020 - 4 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 31, 2020 - 11 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 16, 2020 - 7 AM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 22, 2020 - 1 AM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 9, 2020 - 7 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 28, 2020- 4 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"November 12, 2020 - 4 AM EST\" included an un-recognized timezone \"EST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"June 14, 2021 - 11 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"June 19, 2021 - 4 AM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"June 28, 2021 - 8 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"July 7, 2021 - 8 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 16, 2021 - 4 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 19, 2021 - 7 AM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 22, 2021 - 2 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 29, 2021 - 4 PM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 9, 2021 - 2 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 14, 2021 - 1 AM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 17, 2021 - 5 PM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 21, 2021 - 11 AM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"June 4, 2022 - 11 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"July 2, 2022 - 5 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 4, 2022 - 5 AM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 8, 2022 - 3 PM MDT\" included an un-recognized timezone \"MDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 19, 2022 - 2 AM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 27, 2022 - 8 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 23, 2022 - 6 AM MDT\" included an un-recognized timezone \"MDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"November 10, 2022 - 4 AM EST\" included an un-recognized timezone \"EST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"June 3, 2023 - 10 AM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 20, 2023 - 11 AM PDT\" included an un-recognized timezone \"PDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 23, 2023 - 11 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 22, 2023 - 10 AM CDT\" included an un-recognized timezone \"CDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"August 30, 2023 - 8 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 16, 2023 - 11 PM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"September 23, 2023 - 8 AM EDT\" included an un-recognized timezone \"EDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 3, 2023 - 8 AM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 10, 2023 - 6 PM MDT\" included an un-recognized timezone \"MDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n",
      "C:\\Users\\mobility\\AppData\\Local\\Temp\\ipykernel_15412\\2150577891.py:8: FutureWarning: Parsed string \"October 22, 2023 - 5 AM AST\" included an un-recognized timezone \"AST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  return pd.to_datetime(date_str, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### Config ###\n",
    "df = pd.read_csv('Cyclone_List_to_Collect_New.csv')\n",
    "# Parse the 'Date Landfall/Closest to Land' to datetime\n",
    "def parse_custom_date(date_str):\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, errors='coerce')\n",
    "    except:\n",
    "        return None\n",
    "df['Date Landfall/Closest to Land'] = df['Date Landfall/Closest to Land'].apply(parse_custom_date)\n",
    "df = df.rename(columns={\"Date Landfall/Closest to Land\": \"LandfallDate\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d45b4-b9e7-48ed-9867-50ccd7f69a61",
   "metadata": {},
   "source": [
    "# Before 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19863da5-4bce-4982-b2b7-7f087bb4f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['NAME'].isin(['MATTHEW'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44757f1-d316-4a00-821d-1aef9f59f166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\MATTHEW_2016_12h.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\MATTHEW_2016_12h.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\MATTHEW_2016_12h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\MATTHEW_2016_12h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\MATTHEW_2016_24h.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\MATTHEW_2016_24h.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\MATTHEW_2016_24h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\MATTHEW_2016_24h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\MATTHEW_2016_36h.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\MATTHEW_2016_36h.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\MATTHEW_2016_36h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\MATTHEW_2016_36h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\MATTHEW_2016_48h.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\MATTHEW_2016_48h.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\MATTHEW_2016_48h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\MATTHEW_2016_48h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\MATTHEW_2016_60h.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\MATTHEW_2016_60h.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\MATTHEW_2016_60h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\MATTHEW_2016_60h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\MATTHEW_2016_72h.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\MATTHEW_2016_72h.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\MATTHEW_2016_72h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\MATTHEW_2016_72h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\MATTHEW_2016_84h.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\MATTHEW_2016_84h.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\MATTHEW_2016_84h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\MATTHEW_2016_84h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\MATTHEW_2016_96h.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\MATTHEW_2016_96h.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\MATTHEW_2016_96h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\MATTHEW_2016_96h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\MATTHEW_2016_108h.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\MATTHEW_2016_108h.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\MATTHEW_2016_108h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\MATTHEW_2016_108h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\MATTHEW_2016_120h.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\MATTHEW_2016_120h.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\MATTHEW_2016_120h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\MATTHEW_2016_120h.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "\n",
    "# --- Ensure Output Folders Exist ---\n",
    "folders = {\n",
    "    \"5W\": \"Cyclone Graphics Archive Uncertainty Cone\",\n",
    "    \"PROB34\": \"Cyclone Graphics Archive Wind\",\n",
    "    \"public\": \"Cyclone Text Archive Advisory\",\n",
    "    \"wndprb\": \"Cyclone Text Archive Wind\"\n",
    "}\n",
    "for folder in folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# --- Loop Over Cyclone Records ---\n",
    "for row in df.itertuples(index=False):\n",
    "    year = row.SEASON\n",
    "    storm_id = row.storm_id.upper()\n",
    "    name = row.NAME.upper()\n",
    "    landfall_time = row.LandfallDate\n",
    "\n",
    "    for offset_hr in range(12, 121, 12):  # 12h to 120h (5 days) before\n",
    "        target_time = landfall_time - timedelta(hours=offset_hr)\n",
    "        suffix = f\"_{offset_hr}h\"\n",
    "\n",
    "        # --- GRAPHICS DOWNLOAD ---\n",
    "        for code, folder in [(\"5W\", folders[\"5W\"]), (\"PROB34\", folders[\"PROB34\"])]:\n",
    "            base_url = f\"https://www.nhc.noaa.gov/archive/{year}/graphics/{storm_id.lower()}/\"\n",
    "            try:\n",
    "                res = requests.get(base_url)\n",
    "                res.raise_for_status()\n",
    "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "                file_prefix = f\"{storm_id}{year}_{code}\"\n",
    "                file_entries = []\n",
    "\n",
    "                for tr in soup.find_all(\"tr\"):\n",
    "                    cols = tr.find_all(\"td\")\n",
    "                    if len(cols) >= 3:\n",
    "                        a_tag = cols[1].find(\"a\")\n",
    "                        time_text = cols[2].text.strip()\n",
    "                        if a_tag and a_tag[\"href\"].startswith(file_prefix):\n",
    "                            try:\n",
    "                                mod_time = datetime.strptime(time_text, \"%Y-%m-%d %H:%M\")\n",
    "                                url = urljoin(base_url, a_tag[\"href\"])\n",
    "                                file_entries.append((url, mod_time))\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "                if file_entries:\n",
    "                    url, file_time = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "                    time_diff = abs(file_time - target_time)\n",
    "                    if time_diff > timedelta(hours=12):\n",
    "                        print(f\"❌ No graphics for {code} {storm_id} {year} {suffix} (Too far from target time)\")\n",
    "                        continue\n",
    "                    r = requests.get(url)\n",
    "                    ext = url.split(\"/\")[-1].split(\".\")[-1]\n",
    "                    filename = os.path.join(folder, f\"{name}_{year}{suffix}.{ext}\")\n",
    "                    with open(filename, \"wb\") as f:\n",
    "                        f.write(r.content)\n",
    "                    print(f\"✅ Saved graphics: {filename}\")\n",
    "                else:\n",
    "                    print(f\"❌ No graphics found for {code} {storm_id} {year} {suffix}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error downloading graphics {code} {storm_id} {year} {suffix}: {e}\")\n",
    "\n",
    "        # --- TEXT DOWNLOAD ---\n",
    "        def download_text(product_type, save_folder):\n",
    "            archive_url = f\"https://www.nhc.noaa.gov/archive/{year}/{storm_id.lower()}/\"\n",
    "            try:\n",
    "                res = requests.get(archive_url)\n",
    "                res.raise_for_status()\n",
    "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "                comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "                file_entries = []\n",
    "                for c in comments:\n",
    "                    if c.strip().startswith(str(year)):\n",
    "                        try:\n",
    "                            time = datetime.strptime(c.strip(), \"%Y%m%d %H%M\")\n",
    "                            a_tag = c.find_next_sibling(\"a\")\n",
    "                            if a_tag and product_type in a_tag[\"href\"]:\n",
    "                                url = urljoin(archive_url, a_tag[\"href\"])\n",
    "                                file_entries.append((url, time))\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                if file_entries:\n",
    "                    url, file_time = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "                    time_diff = abs(file_time - target_time)\n",
    "                    if time_diff > timedelta(hours=12):\n",
    "                        print(f\"❌ No text advisory for {product_type} {storm_id} {year} {suffix} (Too far from target time)\")\n",
    "                        return\n",
    "\n",
    "                    page = requests.get(url)\n",
    "                    soup_detail = BeautifulSoup(page.text, \"html.parser\")\n",
    "                    pre_tag = soup_detail.select_one(\"div.textbackground div.textproduct pre\")\n",
    "                    if pre_tag:\n",
    "                        filename = os.path.join(save_folder, f\"{name}_{year}{suffix}.txt\")\n",
    "                        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(pre_tag.get_text())\n",
    "                        print(f\"✅ Saved text: {filename}\")\n",
    "                    else:\n",
    "                        print(f\"❌ No <pre> for {product_type} {storm_id} {year} {suffix}\")\n",
    "                else:\n",
    "                    print(f\"❌ No text advisory for {product_type} {storm_id} {year} {suffix}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error downloading text {product_type} {storm_id} {year} {suffix}: {e}\")\n",
    "\n",
    "        download_text(\"public\", folders[\"public\"])\n",
    "        download_text(\"wndprb\", folders[\"wndprb\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288753e8-1036-49cf-a645-d7e34dee6b61",
   "metadata": {},
   "source": [
    "# After 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c445929b-b382-4b9c-a40c-73e6da2bc5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['SEASON']>2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5feafab6-2eb6-454e-8450-05bf3dd1d04c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\IRMA_2017_12h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\IRMA_2017_12h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\IRMA_2017_12h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\IRMA_2017_12h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\IRMA_2017_24h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\IRMA_2017_24h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\IRMA_2017_24h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\IRMA_2017_24h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\IRMA_2017_36h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\IRMA_2017_36h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\IRMA_2017_36h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\IRMA_2017_36h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\IRMA_2017_48h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\IRMA_2017_48h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\IRMA_2017_48h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\IRMA_2017_48h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\IRMA_2017_60h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\IRMA_2017_60h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\IRMA_2017_60h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\IRMA_2017_60h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\IRMA_2017_72h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\IRMA_2017_72h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\IRMA_2017_72h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\IRMA_2017_72h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\IRMA_2017_84h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\IRMA_2017_84h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\IRMA_2017_84h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\IRMA_2017_84h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\IRMA_2017_96h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\IRMA_2017_96h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\IRMA_2017_96h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\IRMA_2017_96h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\IRMA_2017_108h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\IRMA_2017_108h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\IRMA_2017_108h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\IRMA_2017_108h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\IRMA_2017_120h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\IRMA_2017_120h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\IRMA_2017_120h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\IRMA_2017_120h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ISAIAS_2020_12h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ISAIAS_2020_12h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ISAIAS_2020_12h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ISAIAS_2020_12h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ISAIAS_2020_24h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ISAIAS_2020_24h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ISAIAS_2020_24h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ISAIAS_2020_24h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ISAIAS_2020_36h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ISAIAS_2020_36h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ISAIAS_2020_36h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ISAIAS_2020_36h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ISAIAS_2020_48h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ISAIAS_2020_48h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ISAIAS_2020_48h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ISAIAS_2020_48h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ISAIAS_2020_60h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ISAIAS_2020_60h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ISAIAS_2020_60h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ISAIAS_2020_60h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ISAIAS_2020_72h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ISAIAS_2020_72h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ISAIAS_2020_72h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ISAIAS_2020_72h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ISAIAS_2020_84h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ISAIAS_2020_84h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ISAIAS_2020_84h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ISAIAS_2020_84h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ISAIAS_2020_96h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ISAIAS_2020_96h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ISAIAS_2020_96h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ISAIAS_2020_96h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ISAIAS_2020_108h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ISAIAS_2020_108h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ISAIAS_2020_108h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ISAIAS_2020_108h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ISAIAS_2020_120h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ISAIAS_2020_120h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ISAIAS_2020_120h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ISAIAS_2020_120h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\LAURA_2020_12h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\LAURA_2020_12h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\LAURA_2020_12h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\LAURA_2020_12h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\LAURA_2020_24h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\LAURA_2020_24h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\LAURA_2020_24h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\LAURA_2020_24h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\LAURA_2020_36h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\LAURA_2020_36h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\LAURA_2020_36h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\LAURA_2020_36h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\LAURA_2020_48h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\LAURA_2020_48h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\LAURA_2020_48h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\LAURA_2020_48h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\LAURA_2020_60h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\LAURA_2020_60h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\LAURA_2020_60h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\LAURA_2020_60h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\LAURA_2020_72h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\LAURA_2020_72h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\LAURA_2020_72h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\LAURA_2020_72h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\LAURA_2020_84h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\LAURA_2020_84h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\LAURA_2020_84h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\LAURA_2020_84h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\LAURA_2020_96h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\LAURA_2020_96h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\LAURA_2020_96h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\LAURA_2020_96h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\LAURA_2020_108h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\LAURA_2020_108h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\LAURA_2020_108h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\LAURA_2020_108h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\LAURA_2020_120h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\LAURA_2020_120h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\LAURA_2020_120h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\LAURA_2020_120h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ETA_2020_12h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ETA_2020_12h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ETA_2020_12h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ETA_2020_12h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ETA_2020_24h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ETA_2020_24h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ETA_2020_24h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ETA_2020_24h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ETA_2020_36h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ETA_2020_36h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ETA_2020_36h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ETA_2020_36h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ETA_2020_48h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ETA_2020_48h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ETA_2020_48h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ETA_2020_48h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ETA_2020_60h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ETA_2020_60h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ETA_2020_60h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ETA_2020_60h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ETA_2020_72h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ETA_2020_72h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ETA_2020_72h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ETA_2020_72h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ETA_2020_84h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ETA_2020_84h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ETA_2020_84h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ETA_2020_84h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ETA_2020_96h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ETA_2020_96h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ETA_2020_96h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ETA_2020_96h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ETA_2020_108h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ETA_2020_108h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ETA_2020_108h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ETA_2020_108h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ETA_2020_120h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ETA_2020_120h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ETA_2020_120h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ETA_2020_120h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ELSA_2021_12h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ELSA_2021_12h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ELSA_2021_12h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ELSA_2021_12h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ELSA_2021_24h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ELSA_2021_24h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ELSA_2021_24h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ELSA_2021_24h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ELSA_2021_36h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ELSA_2021_36h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ELSA_2021_36h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ELSA_2021_36h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ELSA_2021_48h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ELSA_2021_48h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ELSA_2021_48h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ELSA_2021_48h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ELSA_2021_60h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ELSA_2021_60h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ELSA_2021_60h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ELSA_2021_60h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ELSA_2021_72h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ELSA_2021_72h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ELSA_2021_72h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ELSA_2021_72h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ELSA_2021_84h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ELSA_2021_84h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ELSA_2021_84h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ELSA_2021_84h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ELSA_2021_96h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ELSA_2021_96h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ELSA_2021_96h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ELSA_2021_96h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ELSA_2021_108h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ELSA_2021_108h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ELSA_2021_108h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ELSA_2021_108h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ELSA_2021_120h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ELSA_2021_120h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ELSA_2021_120h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ELSA_2021_120h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\FRED_2021_12h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\FRED_2021_12h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\FRED_2021_12h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\FRED_2021_12h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\FRED_2021_24h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\FRED_2021_24h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\FRED_2021_24h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\FRED_2021_24h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\FRED_2021_36h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\FRED_2021_36h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\FRED_2021_36h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\FRED_2021_36h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\FRED_2021_48h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\FRED_2021_48h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\FRED_2021_48h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\FRED_2021_48h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\FRED_2021_60h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\FRED_2021_60h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\FRED_2021_60h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\FRED_2021_60h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\FRED_2021_72h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\FRED_2021_72h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\FRED_2021_72h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\FRED_2021_72h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\FRED_2021_84h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\FRED_2021_84h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\FRED_2021_84h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\FRED_2021_84h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\FRED_2021_96h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\FRED_2021_96h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\FRED_2021_96h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\FRED_2021_96h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\FRED_2021_108h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\FRED_2021_108h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\FRED_2021_108h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\FRED_2021_108h.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\FRED_2021_120h.png\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\FRED_2021_120h.png\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\FRED_2021_120h.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\FRED_2021_120h.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "\n",
    "# --- Ensure Output Folders Exist ---\n",
    "folders = {\n",
    "    \"5day\": \"Cyclone Graphics Archive Uncertainty Cone\",\n",
    "    \"wind_probs_34\": \"Cyclone Graphics Archive Wind\",\n",
    "    \"public\": \"Cyclone Text Archive Advisory\",\n",
    "    \"wndprb\": \"Cyclone Text Archive Wind\"\n",
    "}\n",
    "for folder in folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# --- Loop Over Cyclone Records ---\n",
    "for row in df.itertuples(index=False):\n",
    "    year = row.SEASON\n",
    "    storm_id = row.storm_id[:8].upper()\n",
    "    storm_id1 = row.storm_id[:8].upper()\n",
    "    storm_id2 = row.storm_id[:4].lower()\n",
    "    name = row.NAME.upper()\n",
    "    landfall_time = row.LandfallDate\n",
    "\n",
    "    for offset_hr in range(12, 121, 12):  # 12h to 120h (5 days) before\n",
    "        target_time = landfall_time - timedelta(hours=offset_hr)\n",
    "        suffix = f\"_{offset_hr}h\"\n",
    "\n",
    "        # --- GRAPHICS DOWNLOAD ---\n",
    "        for code, folder in [(\"5day\", folders[\"5day\"]), (\"wind_probs_34\", folders[\"wind_probs_34\"])]:\n",
    "            base_url = f\"https://www.nhc.noaa.gov/archive/{year}/graphics/{storm_id}/\"\n",
    "            try:\n",
    "                res = requests.get(base_url)\n",
    "                res.raise_for_status()\n",
    "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "                \n",
    "                file_prefix = f\"{storm_id1}_{code}\"\n",
    "                file_entries = []\n",
    "\n",
    "                for tr in soup.find_all(\"tr\"):\n",
    "                    cols = tr.find_all(\"td\")\n",
    "                    if len(cols) >= 3:\n",
    "                        a_tag = cols[1].find(\"a\")\n",
    "                        time_text = cols[2].text.strip()\n",
    "                        if a_tag and a_tag[\"href\"].startswith(file_prefix):\n",
    "                            try:\n",
    "                                mod_time = datetime.strptime(time_text, \"%Y-%m-%d %H:%M\")\n",
    "                                url = urljoin(base_url, a_tag[\"href\"])\n",
    "                                file_entries.append((url, mod_time))\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "                if file_entries:\n",
    "                    url, file_time = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "                    time_diff = abs(file_time - target_time)\n",
    "                    if time_diff > timedelta(hours=12):\n",
    "                        print(f\"❌ No graphics for {code} {storm_id} {year} {suffix} (Too far from target time)\")\n",
    "                        continue\n",
    "                    r = requests.get(url)\n",
    "                    ext = url.split(\"/\")[-1].split(\".\")[-1]\n",
    "                    filename = os.path.join(folder, f\"{name}_{year}{suffix}.{ext}\")\n",
    "                    with open(filename, \"wb\") as f:\n",
    "                        f.write(r.content)\n",
    "                    print(f\"✅ Saved graphics: {filename}\")\n",
    "                else:\n",
    "                    print(f\"❌ No graphics found for {code} {storm_id} {year} {suffix}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error downloading graphics {code} {storm_id} {year} {suffix}: {e}\")\n",
    "\n",
    "        # --- TEXT DOWNLOAD ---\n",
    "        def download_text(product_type, save_folder):\n",
    "            archive_url = f\"https://www.nhc.noaa.gov/archive/{year}/{storm_id2}/\"\n",
    "            try:\n",
    "                res = requests.get(archive_url)\n",
    "                res.raise_for_status()\n",
    "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "                comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "                file_entries = []\n",
    "                for c in comments:\n",
    "                    if c.strip().startswith(str(year)):\n",
    "                        try:\n",
    "                            time = datetime.strptime(c.strip(), \"%Y%m%d %H%M\")\n",
    "                            a_tag = c.find_next_sibling(\"a\")\n",
    "                            if a_tag and product_type in a_tag[\"href\"]:\n",
    "                                url = urljoin(archive_url, a_tag[\"href\"])\n",
    "                                file_entries.append((url, time))\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                if file_entries:\n",
    "                    url, file_time = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "                    time_diff = abs(file_time - target_time)\n",
    "                    if time_diff > timedelta(hours=12):\n",
    "                        print(f\"❌ No text advisory for {product_type} {storm_id} {year} {suffix} (Too far from target time)\")\n",
    "                        return\n",
    "\n",
    "                    page = requests.get(url)\n",
    "                    soup_detail = BeautifulSoup(page.text, \"html.parser\")\n",
    "                    pre_tag = soup_detail.select_one(\"div.textbackground div.textproduct pre\")\n",
    "                    if pre_tag:\n",
    "                        filename = os.path.join(save_folder, f\"{name}_{year}{suffix}.txt\")\n",
    "                        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(pre_tag.get_text())\n",
    "                        print(f\"✅ Saved text: {filename}\")\n",
    "                    else:\n",
    "                        print(f\"❌ No <pre> for {product_type} {storm_id} {year} {suffix}\")\n",
    "                else:\n",
    "                    print(f\"❌ No text advisory for {product_type} {storm_id} {year} {suffix}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error downloading text {product_type} {storm_id} {year} {suffix}: {e}\")\n",
    "\n",
    "        download_text(\"public\", folders[\"public\"])\n",
    "        download_text(\"wndprb\", folders[\"wndprb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e00a3-0611-42c2-9c73-bac71cd3b92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdc599-ffc1-4be1-b128-d4fd1aa20078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d53c9-8657-4f22-b4e5-03527f30d182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d62aa8-0339-4870-a1e1-5564f1ffd996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1cb86d8b-7638-4f0c-b5a5-48f4ce503f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SID</th>\n",
       "      <th>START_DATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>storm_id</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>LandfallDate</th>\n",
       "      <th>Wind_Speed_1_Day</th>\n",
       "      <th>Wind_Speed_2_Day</th>\n",
       "      <th>Image_1_Day</th>\n",
       "      <th>Image_2_Day</th>\n",
       "      <th>Wind_Speed_Landfall/ Closest_To_Land</th>\n",
       "      <th>Second_Landfall</th>\n",
       "      <th>Third_Landfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2022154N21273</td>\n",
       "      <td>6/2/2022</td>\n",
       "      <td>ALEX</td>\n",
       "      <td>AL012022/</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-06-04 11:00:00</td>\n",
       "      <td>June 3, 2022 - 9:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>June 3, 2022 - 10 AM CDT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2022183N32280</td>\n",
       "      <td>7/1/2022</td>\n",
       "      <td>COLIN</td>\n",
       "      <td>AL032022/</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-07-02 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>July 2, 2022 - 9:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2022246N18301</td>\n",
       "      <td>9/2/2022</td>\n",
       "      <td>EARL</td>\n",
       "      <td>AL062022/</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-09-04 05:00:00</td>\n",
       "      <td>September 3, 2022 - 3:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>September 3, 2022 - 5 AM AST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2022248N14259</td>\n",
       "      <td>9/4/2022</td>\n",
       "      <td>KAY</td>\n",
       "      <td>EP122022/</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-09-08 15:00:00</td>\n",
       "      <td>September 7, 2022 - 15:00 UTC</td>\n",
       "      <td>September 6, 2022 - 15:00 UTC</td>\n",
       "      <td>September 7, 2022 - 3 PM MDT</td>\n",
       "      <td>September 6, 2022 - 3 PM MDT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2022257N16312</td>\n",
       "      <td>9/14/2022</td>\n",
       "      <td>FIONA</td>\n",
       "      <td>AL072022/</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-09-19 02:00:00</td>\n",
       "      <td>September 18, 2022 - 3:00 UTC</td>\n",
       "      <td>September 17, 2022 - 3:00 UTC</td>\n",
       "      <td>September 18, 2022 - 2 AM AST</td>\n",
       "      <td>September 17, 2022 - 2 AM AST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>September 24, 2022 - 5 AM AST</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2022266N12294</td>\n",
       "      <td>9/22/2022</td>\n",
       "      <td>IAN</td>\n",
       "      <td>AL092022/</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-09-27 08:00:00</td>\n",
       "      <td>September 26, 2022 - 9:00 UTC</td>\n",
       "      <td>September 25, 2022 - 9:00 UTC</td>\n",
       "      <td>September 26, 2022 - 8 AM EDT</td>\n",
       "      <td>September 25, 2022 - 8 AM EDT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>September 28, 2022 - 5 PM EDT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2022293N15259</td>\n",
       "      <td>10/20/2022</td>\n",
       "      <td>ROSLYN</td>\n",
       "      <td>EP192022/</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-10-23 06:00:00</td>\n",
       "      <td>October 22, 2022 - 9:00 UTC</td>\n",
       "      <td>October 21, 2022 - 9:00 UTC</td>\n",
       "      <td>October 22, 2022 - 6 AM MDT</td>\n",
       "      <td>October 21, 2022 - 7 AM CDT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2022311N21293</td>\n",
       "      <td>11/6/2022</td>\n",
       "      <td>NICOLE</td>\n",
       "      <td>AL172022/</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-11-10 04:00:00</td>\n",
       "      <td>November 9, 2022 - 3:00 UTC</td>\n",
       "      <td>November 8, 2022 - 3:00 UTC</td>\n",
       "      <td>November 9, 2022 - 4 AM EST</td>\n",
       "      <td>November 8, 2022 - 4 AM EST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              SID  START_DATE    NAME   storm_id  SEASON         LandfallDate  \\\n",
       "71  2022154N21273    6/2/2022    ALEX  AL012022/    2022  2022-06-04 11:00:00   \n",
       "72  2022183N32280    7/1/2022   COLIN  AL032022/    2022  2022-07-02 05:00:00   \n",
       "73  2022246N18301    9/2/2022    EARL  AL062022/    2022  2022-09-04 05:00:00   \n",
       "74  2022248N14259    9/4/2022     KAY  EP122022/    2022  2022-09-08 15:00:00   \n",
       "75  2022257N16312   9/14/2022   FIONA  AL072022/    2022  2022-09-19 02:00:00   \n",
       "76  2022266N12294   9/22/2022     IAN  AL092022/    2022  2022-09-27 08:00:00   \n",
       "77  2022293N15259  10/20/2022  ROSLYN  EP192022/    2022  2022-10-23 06:00:00   \n",
       "78  2022311N21293   11/6/2022  NICOLE  AL172022/    2022  2022-11-10 04:00:00   \n",
       "\n",
       "                 Wind_Speed_1_Day               Wind_Speed_2_Day  \\\n",
       "71        June 3, 2022 - 9:00 UTC                            NaN   \n",
       "72                            NaN                            NaN   \n",
       "73   September 3, 2022 - 3:00 UTC                            NaN   \n",
       "74  September 7, 2022 - 15:00 UTC  September 6, 2022 - 15:00 UTC   \n",
       "75  September 18, 2022 - 3:00 UTC  September 17, 2022 - 3:00 UTC   \n",
       "76  September 26, 2022 - 9:00 UTC  September 25, 2022 - 9:00 UTC   \n",
       "77    October 22, 2022 - 9:00 UTC    October 21, 2022 - 9:00 UTC   \n",
       "78    November 9, 2022 - 3:00 UTC    November 8, 2022 - 3:00 UTC   \n",
       "\n",
       "                      Image_1_Day                    Image_2_Day  \\\n",
       "71       June 3, 2022 - 10 AM CDT                            NaN   \n",
       "72                            NaN                            NaN   \n",
       "73   September 3, 2022 - 5 AM AST                            NaN   \n",
       "74   September 7, 2022 - 3 PM MDT   September 6, 2022 - 3 PM MDT   \n",
       "75  September 18, 2022 - 2 AM AST  September 17, 2022 - 2 AM AST   \n",
       "76  September 26, 2022 - 8 AM EDT  September 25, 2022 - 8 AM EDT   \n",
       "77    October 22, 2022 - 6 AM MDT    October 21, 2022 - 7 AM CDT   \n",
       "78    November 9, 2022 - 4 AM EST    November 8, 2022 - 4 AM EST   \n",
       "\n",
       "   Wind_Speed_Landfall/ Closest_To_Land                Second_Landfall  \\\n",
       "71                                  NaN                            NaN   \n",
       "72              July 2, 2022 - 9:00 UTC                            NaN   \n",
       "73                                  NaN                            NaN   \n",
       "74                                  NaN                            NaN   \n",
       "75                                  NaN  September 24, 2022 - 5 AM AST   \n",
       "76                                  NaN  September 28, 2022 - 5 PM EDT   \n",
       "77                                  NaN                            NaN   \n",
       "78                                  NaN                            NaN   \n",
       "\n",
       "   Third_Landfall  \n",
       "71            NaN  \n",
       "72            NaN  \n",
       "73            NaN  \n",
       "74            NaN  \n",
       "75            NaN  \n",
       "76            NaN  \n",
       "77            NaN  \n",
       "78            NaN  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['SEASON']==2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d12c2a2-b762-4ab2-9eb1-ac6899f018d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e2ff4d-4737-434f-aaf3-6a89bd11c26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00e2f0-e0f3-4a24-9a2e-6fcc0f94be3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba79ea-1cb4-4fe2-a076-02bd460116ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59b737-ed1c-48d9-9672-6d5bd8fb4c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3f0c1-67ff-4a7c-9f0f-ff8fd5c30fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e8741-1a0f-433a-b71e-06faf88fdefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8aab5-00e3-4b47-aaaf-773870f75a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2046e812-f7ef-437b-b57a-f6ad3f9d7891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ANA_1.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ANA_1.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ANA_1.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ANA_1.txt\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ANA_2.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ANA_2.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ANA_2.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ANA_2.txt\n",
      "❌ No graphics for 5W AL01 2015 Day -3 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL01 2015 Day -3 (Too far from target time)\n",
      "❌ No text advisory for public AL01 2015 Day -3 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL01 2015 Day -3 (Too far from target time)\n",
      "❌ No graphics for 5W AL01 2015 Day -4 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL01 2015 Day -4 (Too far from target time)\n",
      "❌ No text advisory for public AL01 2015 Day -4 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL01 2015 Day -4 (Too far from target time)\n",
      "❌ No graphics for 5W AL01 2015 Day -5 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL01 2015 Day -5 (Too far from target time)\n",
      "❌ No text advisory for public AL01 2015 Day -5 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL01 2015 Day -5 (Too far from target time)\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\BILL_1.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\BILL_1.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\BILL_1.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\BILL_1.txt\n",
      "❌ No graphics for 5W AL02 2015 Day -2 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL02 2015 Day -2 (Too far from target time)\n",
      "❌ No text advisory for public AL02 2015 Day -2 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL02 2015 Day -2 (Too far from target time)\n",
      "❌ No graphics for 5W AL02 2015 Day -3 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL02 2015 Day -3 (Too far from target time)\n",
      "❌ No text advisory for public AL02 2015 Day -3 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL02 2015 Day -3 (Too far from target time)\n",
      "❌ No graphics for 5W AL02 2015 Day -4 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL02 2015 Day -4 (Too far from target time)\n",
      "❌ No text advisory for public AL02 2015 Day -4 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL02 2015 Day -4 (Too far from target time)\n",
      "❌ No graphics for 5W AL02 2015 Day -5 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL02 2015 Day -5 (Too far from target time)\n",
      "❌ No text advisory for public AL02 2015 Day -5 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL02 2015 Day -5 (Too far from target time)\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\DOLORES_1.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\DOLORES_1.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\DOLORES_1.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\DOLORES_1.txt\n",
      "❌ No graphics for 5W EP05 2015 Day -2 (Too far from target time)\n",
      "❌ No graphics for PROB34 EP05 2015 Day -2 (Too far from target time)\n",
      "❌ No text advisory for public EP05 2015 Day -2 (Too far from target time)\n",
      "❌ No text advisory for wndprb EP05 2015 Day -2 (Too far from target time)\n",
      "❌ No graphics for 5W EP05 2015 Day -3 (Too far from target time)\n",
      "❌ No graphics for PROB34 EP05 2015 Day -3 (Too far from target time)\n",
      "❌ No text advisory for public EP05 2015 Day -3 (Too far from target time)\n",
      "❌ No text advisory for wndprb EP05 2015 Day -3 (Too far from target time)\n",
      "❌ No graphics for 5W EP05 2015 Day -4 (Too far from target time)\n",
      "❌ No graphics for PROB34 EP05 2015 Day -4 (Too far from target time)\n",
      "❌ No text advisory for public EP05 2015 Day -4 (Too far from target time)\n",
      "❌ No text advisory for wndprb EP05 2015 Day -4 (Too far from target time)\n",
      "❌ No graphics for 5W EP05 2015 Day -5 (Too far from target time)\n",
      "❌ No graphics for PROB34 EP05 2015 Day -5 (Too far from target time)\n",
      "❌ No text advisory for public EP05 2015 Day -5 (Too far from target time)\n",
      "❌ No text advisory for wndprb EP05 2015 Day -5 (Too far from target time)\n",
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\CLAUDETTE_1.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\CLAUDETTE_1.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\CLAUDETTE_1.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\CLAUDETTE_1.txt\n",
      "❌ No graphics for 5W AL03 2015 Day -2 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL03 2015 Day -2 (Too far from target time)\n",
      "❌ No text advisory for public AL03 2015 Day -2 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL03 2015 Day -2 (Too far from target time)\n",
      "❌ No graphics for 5W AL03 2015 Day -3 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL03 2015 Day -3 (Too far from target time)\n",
      "❌ No text advisory for public AL03 2015 Day -3 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL03 2015 Day -3 (Too far from target time)\n",
      "❌ No graphics for 5W AL03 2015 Day -4 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL03 2015 Day -4 (Too far from target time)\n",
      "❌ No text advisory for public AL03 2015 Day -4 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL03 2015 Day -4 (Too far from target time)\n",
      "❌ No graphics for 5W AL03 2015 Day -5 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL03 2015 Day -5 (Too far from target time)\n",
      "❌ No text advisory for public AL03 2015 Day -5 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL03 2015 Day -5 (Too far from target time)\n",
      "❌ No graphics for 5W AL12 2015 Day -1 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL12 2015 Day -1 (Too far from target time)\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\KATE_1.txt\n",
      "❌ No text advisory for wndprb AL12 2015 Day -1 (Too far from target time)\n",
      "❌ No graphics for 5W AL12 2015 Day -2 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL12 2015 Day -2 (Too far from target time)\n",
      "❌ No text advisory for public AL12 2015 Day -2 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL12 2015 Day -2 (Too far from target time)\n",
      "❌ No graphics for 5W AL12 2015 Day -3 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL12 2015 Day -3 (Too far from target time)\n",
      "❌ No text advisory for public AL12 2015 Day -3 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL12 2015 Day -3 (Too far from target time)\n",
      "❌ No graphics for 5W AL12 2015 Day -4 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL12 2015 Day -4 (Too far from target time)\n",
      "❌ No text advisory for public AL12 2015 Day -4 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL12 2015 Day -4 (Too far from target time)\n",
      "❌ No graphics for 5W AL12 2015 Day -5 (Too far from target time)\n",
      "❌ No graphics for PROB34 AL12 2015 Day -5 (Too far from target time)\n",
      "❌ No text advisory for public AL12 2015 Day -5 (Too far from target time)\n",
      "❌ No text advisory for wndprb AL12 2015 Day -5 (Too far from target time)\n"
     ]
    }
   ],
   "source": [
    "# Every one day\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "\n",
    "# --- Ensure Output Folders Exist ---\n",
    "folders = {\n",
    "    \"5W\": \"Cyclone Graphics Archive Uncertainty Cone\",\n",
    "    \"PROB34\": \"Cyclone Graphics Archive Wind\",\n",
    "    \"public\": \"Cyclone Text Archive Advisory\",\n",
    "    \"wndprb\": \"Cyclone Text Archive Wind\"\n",
    "}\n",
    "for folder in folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# --- Loop Over Cyclone Records ---\n",
    "for row in df.itertuples(index=False):\n",
    "    year = row.SEASON\n",
    "    storm_id = row.storm_id.upper()\n",
    "    name = row.NAME.upper()\n",
    "    landfall_time = row.LandfallDate\n",
    "\n",
    "    for day_offset in range(1, 6):  # 1 to 5 days before\n",
    "        target_time = landfall_time - timedelta(days=day_offset)\n",
    "        suffix = f\"_{day_offset}\"\n",
    "\n",
    "        # --- GRAPHICS DOWNLOAD ---\n",
    "        for code, folder in [(\"5W\", folders[\"5W\"]), (\"PROB34\", folders[\"PROB34\"])]:\n",
    "            base_url = f\"https://www.nhc.noaa.gov/archive/{year}/graphics/{storm_id.lower()}/\"\n",
    "            try:\n",
    "                res = requests.get(base_url)\n",
    "                res.raise_for_status()\n",
    "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "                file_prefix = f\"{storm_id}{year}_{code}\"\n",
    "                file_entries = []\n",
    "\n",
    "                for tr in soup.find_all(\"tr\"):\n",
    "                    cols = tr.find_all(\"td\")\n",
    "                    if len(cols) >= 3:\n",
    "                        a_tag = cols[1].find(\"a\")\n",
    "                        time_text = cols[2].text.strip()\n",
    "                        if a_tag and a_tag[\"href\"].startswith(file_prefix):\n",
    "                            try:\n",
    "                                mod_time = datetime.strptime(time_text, \"%Y-%m-%d %H:%M\")\n",
    "                                url = urljoin(base_url, a_tag[\"href\"])\n",
    "                                file_entries.append((url, mod_time))\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "                if file_entries:\n",
    "                    url, file_time = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "                    time_diff = abs(file_time - target_time)\n",
    "                    if time_diff > timedelta(hours=12):\n",
    "                        print(f\"❌ No graphics for {code} {storm_id} {year} Day -{day_offset} (Too far from target time)\")\n",
    "                        continue\n",
    "                    r = requests.get(url)\n",
    "                    ext = url.split(\"/\")[-1].split(\".\")[-1]\n",
    "                    filename = os.path.join(folder, f\"{name}{suffix}.{ext}\")\n",
    "                    with open(filename, \"wb\") as f:\n",
    "                        f.write(r.content)\n",
    "                    print(f\"✅ Saved graphics: {filename}\")\n",
    "                else:\n",
    "                    print(f\"❌ No graphics found for {code} {storm_id} {year} Day -{day_offset}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error downloading graphics {code} {storm_id} {year} Day -{day_offset}: {e}\")\n",
    "\n",
    "        # --- TEXT DOWNLOAD ---\n",
    "        def download_text(product_type, save_folder):\n",
    "            archive_url = f\"https://www.nhc.noaa.gov/archive/{year}/{storm_id.lower()}/\"\n",
    "            try:\n",
    "                res = requests.get(archive_url)\n",
    "                res.raise_for_status()\n",
    "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "                comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "                file_entries = []\n",
    "                for c in comments:\n",
    "                    if c.strip().startswith(str(year)):\n",
    "                        try:\n",
    "                            time = datetime.strptime(c.strip(), \"%Y%m%d %H%M\")\n",
    "                            a_tag = c.find_next_sibling(\"a\")\n",
    "                            if a_tag and product_type in a_tag[\"href\"]:\n",
    "                                url = urljoin(archive_url, a_tag[\"href\"])\n",
    "                                file_entries.append((url, time))\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                if file_entries:\n",
    "                    url, file_time = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "                    time_diff = abs(file_time - target_time)\n",
    "                    if time_diff > timedelta(hours=12):\n",
    "                        print(f\"❌ No text advisory for {product_type} {storm_id} {year} Day -{day_offset} (Too far from target time)\")\n",
    "                        return\n",
    "\n",
    "                    page = requests.get(url)\n",
    "                    soup_detail = BeautifulSoup(page.text, \"html.parser\")\n",
    "                    pre_tag = soup_detail.select_one(\"div.textbackground div.textproduct pre\")\n",
    "                    if pre_tag:\n",
    "                        filename = os.path.join(save_folder, f\"{name}{suffix}.txt\")\n",
    "                        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(pre_tag.get_text())\n",
    "                        print(f\"✅ Saved text: {filename}\")\n",
    "                    else:\n",
    "                        print(f\"❌ No <pre> for {product_type} {storm_id} {year} Day -{day_offset}\")\n",
    "                else:\n",
    "                    print(f\"❌ No text advisory for {product_type} {storm_id} {year} Day -{day_offset}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error downloading text {product_type} {storm_id} {year} Day -{day_offset}: {e}\")\n",
    "\n",
    "        download_text(\"public\", folders[\"public\"])\n",
    "        download_text(\"wndprb\", folders[\"wndprb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a491ae9-c652-4b20-9e33-b6218e450213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved graphics: Cyclone Graphics Archive Uncertainty Cone\\ANA.GIF\n",
      "✅ Saved graphics: Cyclone Graphics Archive Wind\\ANA_wind.GIF\n",
      "✅ Saved text: Cyclone Text Archive Advisory\\ANA_advisory.txt\n",
      "✅ Saved text: Cyclone Text Archive Wind\\ANA_wind.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "\n",
    "# --- YOUR STORM DATAFRAME ---\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'SEASON': 2015,\n",
    "        'storm_id': 'AL01',\n",
    "        'NAME': 'ANA',\n",
    "        'Date Landfall/Closest to Land': datetime.strptime(\"2015-05-10 17:55\", \"%Y-%m-%d %H:%M\")\n",
    "    }\n",
    "])\n",
    "\n",
    "# --- OUTPUT FOLDERS ---\n",
    "folders = {\n",
    "    \"5W\": \"Cyclone Graphics Archive Uncertainty Cone\",\n",
    "    \"PROB34\": \"Cyclone Graphics Archive Wind\",\n",
    "    \"public\": \"Cyclone Text Archive Advisory\",\n",
    "    \"wndprb\": \"Cyclone Text Archive Wind\"\n",
    "}\n",
    "for folder in folders.values():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "for row in df.itertuples(index=False):\n",
    "    year = row.SEASON\n",
    "    storm_id = row.storm_id.upper()\n",
    "    name = row.NAME.upper()\n",
    "    target_time = row._3  # Date Landfall/Closest to Land\n",
    "\n",
    "    # --- GRAPHICS DOWNLOAD (5W and PROB34) ---\n",
    "    for code, folder in [(\"5W\", folders[\"5W\"]), (\"PROB34\", folders[\"PROB34\"])]:\n",
    "        base_url = f\"https://www.nhc.noaa.gov/archive/{year}/graphics/{storm_id.lower()}/\"\n",
    "        try:\n",
    "            res = requests.get(base_url)\n",
    "            res.raise_for_status()\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "            file_prefix = f\"{storm_id}{year}_{code}\"\n",
    "            file_entries = []\n",
    "\n",
    "            for tr in soup.find_all(\"tr\"):\n",
    "                cols = tr.find_all(\"td\")\n",
    "                if len(cols) >= 3:\n",
    "                    a_tag = cols[1].find(\"a\")\n",
    "                    time_text = cols[2].text.strip()\n",
    "                    if a_tag and a_tag[\"href\"].startswith(file_prefix):\n",
    "                        try:\n",
    "                            mod_time = datetime.strptime(time_text, \"%Y-%m-%d %H:%M\")\n",
    "                            url = urljoin(base_url, a_tag[\"href\"])\n",
    "                            file_entries.append((url, mod_time))\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "            if file_entries:\n",
    "                url, _ = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "                r = requests.get(url)\n",
    "                ext = url.split(\"/\")[-1].split(\".\")[-1]\n",
    "                suffix = \"_wind\" if \"PROB34\" in code else \"\"\n",
    "                filename = os.path.join(folder, f\"{name}{suffix}.{ext}\")\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                print(f\"✅ Saved graphics: {filename}\")\n",
    "            else:\n",
    "                print(f\"❌ No graphics found for {code} in {storm_id} {year}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error downloading graphics for {code} {storm_id} {year}: {e}\")\n",
    "\n",
    "    # --- TEXT DOWNLOAD (public and wndprb) ---\n",
    "    def download_text(product_type, save_folder, suffix):\n",
    "        archive_url = f\"https://www.nhc.noaa.gov/archive/{year}/{storm_id.lower()}/\"\n",
    "        try:\n",
    "            res = requests.get(archive_url)\n",
    "            res.raise_for_status()\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "            comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "            file_entries = []\n",
    "            for c in comments:\n",
    "                if c.strip().startswith(str(year)):\n",
    "                    try:\n",
    "                        time = datetime.strptime(c.strip(), \"%Y%m%d %H%M\")\n",
    "                        a_tag = c.find_next_sibling(\"a\")\n",
    "                        if a_tag and product_type in a_tag[\"href\"]:\n",
    "                            url = urljoin(archive_url, a_tag[\"href\"])\n",
    "                            file_entries.append((url, time))\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            if file_entries:\n",
    "                url, _ = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "                page = requests.get(url)\n",
    "                soup_detail = BeautifulSoup(page.text, \"html.parser\")\n",
    "                pre_tag = soup_detail.select_one(\"div.textbackground div.textproduct pre\")\n",
    "                if pre_tag:\n",
    "                    filename = os.path.join(save_folder, f\"{name}{suffix}.txt\")\n",
    "                    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(pre_tag.get_text())\n",
    "                    print(f\"✅ Saved text: {filename}\")\n",
    "                else:\n",
    "                    print(f\"❌ No <pre> found for {product_type} in {storm_id} {year}\")\n",
    "            else:\n",
    "                print(f\"❌ No text advisory found for {product_type} in {storm_id} {year}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error downloading text for {product_type} {storm_id} {year}: {e}\")\n",
    "\n",
    "    download_text(\"public\", folders[\"public\"], \"_advisory\")\n",
    "    download_text(\"wndprb\", folders[\"wndprb\"], \"_wind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10def20e-8495-4e05-b6b1-8da07b93105a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002abb28-2025-478a-b5bf-8ea34aa17419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab52a9-9a96-464a-98a4-a19e8f01bf9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba571b43-5784-4128-b881-68fc6c26a692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab4a02a8-f283-4fd5-b546-cd2cd34b3bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the script after execution environment reset\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame input (replace this with actual data)\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'SEASON': 2015,\n",
    "        'storm_id': 'AL01',\n",
    "        'NAME': 'ANA',\n",
    "        'Date Landfall/Closest to Land': datetime.strptime(\"2015-05-10 17:55\", \"%Y-%m-%d %H:%M\")\n",
    "    }\n",
    "])\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(\"Cyclone Graphics Archive Uncertainty Cone\", exist_ok=True)\n",
    "os.makedirs(\"Cyclone Graphics Archive Wind\", exist_ok=True)\n",
    "os.makedirs(\"Cyclone Text Archive Advisory\", exist_ok=True)\n",
    "os.makedirs(\"Cyclone Text Archive Wind\", exist_ok=True)\n",
    "\n",
    "for row in df.itertuples(index=False):\n",
    "    year = row.SEASON\n",
    "    storm_id = row.storm_id.upper()\n",
    "    name = row.NAME\n",
    "    target_time = row._3  # Date Landfall/Closest to Land\n",
    "\n",
    "    # --- GRAPHICS DOWNLOAD ---\n",
    "    product_codes = [(\"5W\", \"Cyclone Graphics Archive Uncertainty Cone\"),\n",
    "                     (\"PROB34\", \"Cyclone Graphics Archive Wind\")]\n",
    "    base_url = f\"https://www.nhc.noaa.gov/archive/{year}/graphics/{storm_id.lower()}/\"\n",
    "\n",
    "    try:\n",
    "        res = requests.get(base_url)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        for product_code, folder in product_codes:\n",
    "            file_prefix = f\"{storm_id}{year}_{product_code}\"\n",
    "            file_entries = []\n",
    "\n",
    "            for tr in soup.find_all(\"tr\"):\n",
    "                cols = tr.find_all(\"td\")\n",
    "                if len(cols) >= 3:\n",
    "                    a_tag = cols[1].find(\"a\")\n",
    "                    mod_time_text = cols[2].text.strip()\n",
    "                    if a_tag and a_tag[\"href\"].startswith(file_prefix):\n",
    "                        file_url = urljoin(base_url, a_tag[\"href\"])\n",
    "                        try:\n",
    "                            mod_time = datetime.strptime(mod_time_text, \"%Y-%m-%d %H:%M\")\n",
    "                            file_entries.append((file_url, mod_time))\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "\n",
    "            if file_entries:\n",
    "                closest_file = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "                r = requests.get(closest_file[0])\n",
    "                filename = os.path.join(folder, closest_file[0].split(\"/\")[-1])\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading graphics for {storm_id} {year}: {e}\")\n",
    "\n",
    "    # --- TEXT ADVISORY DOWNLOAD ---\n",
    "    def download_text_product(product_type, save_folder):\n",
    "        archive_url = f\"https://www.nhc.noaa.gov/archive/{year}/{storm_id.lower()}/\"\n",
    "        try:\n",
    "            res = requests.get(archive_url)\n",
    "            res.raise_for_status()\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "            file_entries = []\n",
    "\n",
    "            for comment in comments:\n",
    "                comment_text = comment.strip()\n",
    "                if comment_text.startswith(str(year)):\n",
    "                    try:\n",
    "                        dt = datetime.strptime(comment_text, \"%Y%m%d %H%M\")\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "                    a_tag = comment.find_next_sibling(\"a\")\n",
    "                    if a_tag and a_tag.get(\"href\"):\n",
    "                        href = a_tag[\"href\"]\n",
    "                        if product_type in href:\n",
    "                            full_url = urljoin(archive_url, href)\n",
    "                            file_entries.append((full_url, dt))\n",
    "\n",
    "            if file_entries:\n",
    "                closest_file = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "                page = requests.get(closest_file[0])\n",
    "                soup_detail = BeautifulSoup(page.text, \"html.parser\")\n",
    "                pre_tag = soup_detail.select_one(\"div.textbackground div.textproduct pre\")\n",
    "                if pre_tag:\n",
    "                    text = pre_tag.get_text()\n",
    "                    filename = closest_file[0].split(\"/\")[-1].split(\"?\")[0].replace(\".shtml\", \".txt\")\n",
    "                    with open(os.path.join(save_folder, filename), \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {product_type} text for {storm_id} {year}: {e}\")\n",
    "\n",
    "    download_text_product(\"public\", \"Cyclone Text Archive Advisory\")\n",
    "    download_text_product(\"wndprb\", \"Cyclone Text Archive Wind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc73049-acd2-4ff4-8d02-5f4385bef137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d3683-c4e5-4653-be2b-8edc5f268e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ccfcabd-44c4-4b28-98f4-eb4657119836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Closest file for 5W: https://www.nhc.noaa.gov/archive/2015/graphics/al01/AL012015_5W_009_0.GIF (Modified: 2015-05-10 02:45:00)\n",
      "✅ Downloaded to: AL012015_5W_009_0.GIF\n",
      "📄 Closest file for PROB34: https://www.nhc.noaa.gov/archive/2015/graphics/al01/AL012015_PROB34_009_F120.GIF (Modified: 2015-05-10 03:03:00)\n",
      "✅ Downloaded to: AL012015_PROB34_009_F120.GIF\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIG ---\n",
    "product_codes = [\"5W\", \"PROB34\"]  # Multiple product codes\n",
    "target_time = datetime.strptime(\"2015-05-10 03:00\", \"%Y-%m-%d %H:%M\")\n",
    "base_url = f\"https://www.nhc.noaa.gov/archive/{year}/graphics/{storm_id.lower()}/\"\n",
    "\n",
    "# --- SCRAPE & DOWNLOAD LOOP ---\n",
    "res = requests.get(base_url)\n",
    "res.raise_for_status()\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "for product_code in product_codes:\n",
    "    file_prefix = f\"{storm_id}{year}_{product_code}\"\n",
    "    file_entries = []\n",
    "\n",
    "    for row in soup.find_all(\"tr\"):\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) >= 3:\n",
    "            a_tag = cols[1].find(\"a\")\n",
    "            mod_time_text = cols[2].text.strip()\n",
    "            if a_tag and a_tag[\"href\"].startswith(file_prefix):\n",
    "                file_url = urljoin(base_url, a_tag[\"href\"])\n",
    "                try:\n",
    "                    mod_time = datetime.strptime(mod_time_text, \"%Y-%m-%d %H:%M\")\n",
    "                    file_entries.append((file_url, mod_time))\n",
    "                except ValueError:\n",
    "                    continue  # skip malformed time entries\n",
    "\n",
    "    if not file_entries:\n",
    "        print(f\"❌ No matching files starting with {file_prefix} found.\")\n",
    "    else:\n",
    "        closest_file = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "        print(f\"📄 Closest file for {product_code}: {closest_file[0]} (Modified: {closest_file[1]})\")\n",
    "\n",
    "        # --- DOWNLOAD ---\n",
    "        r = requests.get(closest_file[0])\n",
    "        filename = closest_file[0].split(\"/\")[-1]\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        print(f\"✅ Downloaded to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c8ee77d-b250-45ea-b995-39d95d3f042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Closest advisory: https://www.nhc.noaa.gov/archive/2015/al01/al012015.public.012.shtml (Issued: 2015-05-10 17:00:00)\n",
      "✅ Advisory text saved to: al012015.public.012.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "storm_id = \"al01\"\n",
    "year = 2015\n",
    "product_type = \"public\"\n",
    "target_time = datetime.strptime(\"2015-05-10 17:55\", \"%Y-%m-%d %H:%M\")\n",
    "archive_url = f\"https://www.nhc.noaa.gov/archive/{year}/{storm_id}/\"\n",
    "\n",
    "# --- FETCH ARCHIVE PAGE ---\n",
    "res = requests.get(archive_url)\n",
    "res.raise_for_status()\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "# --- FIND FILES WITH TIME IN COMMENT ---\n",
    "file_entries = []\n",
    "comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "\n",
    "for comment in comments:\n",
    "    comment_text = comment.strip()\n",
    "    if comment_text.startswith(\"2015\"):\n",
    "        try:\n",
    "            dt = datetime.strptime(comment_text, \"%Y%m%d %H%M\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        a_tag = comment.find_next_sibling(\"a\")\n",
    "        if a_tag and a_tag.get(\"href\"):\n",
    "            href = a_tag[\"href\"]\n",
    "            if product_type in href:\n",
    "                full_url = urljoin(archive_url, href)\n",
    "                file_entries.append((full_url, dt))\n",
    "\n",
    "# --- FIND CLOSEST FILE ---\n",
    "if not file_entries:\n",
    "    print(\"❌ No matching advisories found.\")\n",
    "else:\n",
    "    closest_file = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "    url_to_download = closest_file[0]\n",
    "    print(f\"📄 Closest advisory: {url_to_download} (Issued: {closest_file[1]})\")\n",
    "\n",
    "    # --- DOWNLOAD AND EXTRACT <pre> ---\n",
    "    page = requests.get(url_to_download)\n",
    "    soup_detail = BeautifulSoup(page.text, \"html.parser\")\n",
    "    pre_tag = soup_detail.select_one(\"div.textbackground div.textproduct pre\")\n",
    "\n",
    "    if pre_tag:\n",
    "        text = pre_tag.get_text()\n",
    "        filename = url_to_download.split(\"/\")[-1].split(\"?\")[0].replace(\".shtml\", \".txt\")\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"✅ Advisory text saved to: {filename}\")\n",
    "    else:\n",
    "        print(\"❌ <pre> tag not found in the advisory page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48c35f7c-41a9-4496-82f8-6e2965c6bc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Closest wind probability advisory: https://www.nhc.noaa.gov/archive/2015/al01/al012015.wndprb.011.shtml (Issued: 2015-05-10 15:00:00)\n",
      "✅ Wind probability advisory text saved to: al012015.wndprb.011.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "storm_id = \"al01\"\n",
    "year = 2015\n",
    "product_type = \"wndprb\"  # wind probability product\n",
    "target_time = datetime.strptime(\"2015-05-10 17:55\", \"%Y-%m-%d %H:%M\")\n",
    "archive_url = f\"https://www.nhc.noaa.gov/archive/{year}/{storm_id}/\"\n",
    "\n",
    "# --- FETCH ADVISORY ARCHIVE PAGE ---\n",
    "res = requests.get(archive_url)\n",
    "res.raise_for_status()\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "# --- PARSE COMMENT TIMESTAMPS AND FIND LINKS ---\n",
    "file_entries = []\n",
    "comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "\n",
    "for comment in comments:\n",
    "    comment_text = comment.strip()\n",
    "    if comment_text.startswith(\"2015\"):\n",
    "        try:\n",
    "            dt = datetime.strptime(comment_text, \"%Y%m%d %H%M\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        a_tag = comment.find_next_sibling(\"a\")\n",
    "        if a_tag and a_tag.get(\"href\"):\n",
    "            href = a_tag[\"href\"]\n",
    "            if product_type in href:\n",
    "                full_url = urljoin(archive_url, href)\n",
    "                file_entries.append((full_url, dt))\n",
    "\n",
    "# --- FIND CLOSEST FILE TO TARGET TIME ---\n",
    "if not file_entries:\n",
    "    print(f\"❌ No matching {product_type} advisories found.\")\n",
    "else:\n",
    "    closest_file = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "    url_to_download = closest_file[0]\n",
    "    print(f\"📄 Closest wind probability advisory: {url_to_download} (Issued: {closest_file[1]})\")\n",
    "\n",
    "    # --- DOWNLOAD AND EXTRACT TEXT ---\n",
    "    page = requests.get(url_to_download)\n",
    "    soup_detail = BeautifulSoup(page.text, \"html.parser\")\n",
    "    pre_tag = soup_detail.select_one(\"div.textbackground div.textproduct pre\")\n",
    "\n",
    "    if pre_tag:\n",
    "        text = pre_tag.get_text()\n",
    "        filename = url_to_download.split(\"/\")[-1].split(\"?\")[0].replace(\".shtml\", \".txt\")\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"✅ Wind probability advisory text saved to: {filename}\")\n",
    "    else:\n",
    "        print(\"❌ <pre> tag not found in the wind probability advisory page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8d07189-13a5-49f7-a934-f039a0f7e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the script after execution environment reset\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame input (replace this with actual data)\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'SEASON': 2015,\n",
    "        'storm_id': 'AL01',\n",
    "        'NAME': 'ANA',\n",
    "        'Date Landfall/Closest to Land': datetime.strptime(\"2015-05-10 17:55\", \"%Y-%m-%d %H:%M\")\n",
    "    }\n",
    "])\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(\"Cyclone Graphics Archive Uncertainty Cone\", exist_ok=True)\n",
    "os.makedirs(\"Cyclone Graphics Archive Wind\", exist_ok=True)\n",
    "os.makedirs(\"Cyclone Text Archive Advisory\", exist_ok=True)\n",
    "os.makedirs(\"Cyclone Text Archive Wind\", exist_ok=True)\n",
    "\n",
    "for row in df.itertuples(index=False):\n",
    "    year = row.SEASON\n",
    "    storm_id = row.storm_id.upper()\n",
    "    name = row.NAME\n",
    "    target_time = row._3  # Date Landfall/Closest to Land\n",
    "\n",
    "    # --- GRAPHICS DOWNLOAD ---\n",
    "    product_codes = [(\"5W\", \"Cyclone Graphics Archive Uncertainty Cone\"),\n",
    "                     (\"PROB34\", \"Cyclone Graphics Archive Wind\")]\n",
    "    base_url = f\"https://www.nhc.noaa.gov/archive/{year}/graphics/{storm_id.lower()}/\"\n",
    "\n",
    "    try:\n",
    "        res = requests.get(base_url)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        for product_code, folder in product_codes:\n",
    "            file_prefix = f\"{storm_id}{year}_{product_code}\"\n",
    "            file_entries = []\n",
    "\n",
    "            for tr in soup.find_all(\"tr\"):\n",
    "                cols = tr.find_all(\"td\")\n",
    "                if len(cols) >= 3:\n",
    "                    a_tag = cols[1].find(\"a\")\n",
    "                    mod_time_text = cols[2].text.strip()\n",
    "                    if a_tag and a_tag[\"href\"].startswith(file_prefix):\n",
    "                        file_url = urljoin(base_url, a_tag[\"href\"])\n",
    "                        try:\n",
    "                            mod_time = datetime.strptime(mod_time_text, \"%Y-%m-%d %H:%M\")\n",
    "                            file_entries.append((file_url, mod_time))\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "\n",
    "            if file_entries:\n",
    "                closest_file = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "                r = requests.get(closest_file[0])\n",
    "                filename = os.path.join(folder, closest_file[0].split(\"/\")[-1])\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading graphics for {storm_id} {year}: {e}\")\n",
    "\n",
    "    # --- TEXT ADVISORY DOWNLOAD ---\n",
    "    def download_text_product(product_type, save_folder):\n",
    "        archive_url = f\"https://www.nhc.noaa.gov/archive/{year}/{storm_id.lower()}/\"\n",
    "        try:\n",
    "            res = requests.get(archive_url)\n",
    "            res.raise_for_status()\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "            file_entries = []\n",
    "\n",
    "            for comment in comments:\n",
    "                comment_text = comment.strip()\n",
    "                if comment_text.startswith(str(year)):\n",
    "                    try:\n",
    "                        dt = datetime.strptime(comment_text, \"%Y%m%d %H%M\")\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "                    a_tag = comment.find_next_sibling(\"a\")\n",
    "                    if a_tag and a_tag.get(\"href\"):\n",
    "                        href = a_tag[\"href\"]\n",
    "                        if product_type in href:\n",
    "                            full_url = urljoin(archive_url, href)\n",
    "                            file_entries.append((full_url, dt))\n",
    "\n",
    "            if file_entries:\n",
    "                closest_file = min(file_entries, key=lambda x: abs(x[1] - target_time))\n",
    "                page = requests.get(closest_file[0])\n",
    "                soup_detail = BeautifulSoup(page.text, \"html.parser\")\n",
    "                pre_tag = soup_detail.select_one(\"div.textbackground div.textproduct pre\")\n",
    "                if pre_tag:\n",
    "                    text = pre_tag.get_text()\n",
    "                    filename = closest_file[0].split(\"/\")[-1].split(\"?\")[0].replace(\".shtml\", \".txt\")\n",
    "                    with open(os.path.join(save_folder, filename), \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {product_type} text for {storm_id} {year}: {e}\")\n",
    "\n",
    "    download_text_product(\"public\", \"Cyclone Text Archive Advisory\")\n",
    "    download_text_product(\"wndprb\", \"Cyclone Text Archive Wind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b58aa-9368-4502-8ad0-641cb1ba2362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
