# CyPortQA

> **Anonymous research benchmark for cyclone preparedness in Port Operation (AAAIÂ 2026 submission)**

CyPortQA is the first **multimodal QA benchmark** that evaluates how well multimodal largeâ€‘language models (MLLMs) can
1. understand **tropicalâ€‘cyclone forecasts**,
2. reason about **portâ€‘level impacts**, and
3. recommend **operational strategies**.

The dataset fuses realâ€‘world NOAA hurricane products, USCG portâ€‘condition bulletins, and AISâ€‘derived portâ€‘performance metrics collected from **2015â€¯â€“â€¯2023** to generate **117â€¯kâ€¯+** questionâ€“answer pairs across three task groups:

| Task | Core Ability | Example Question |
|------|--------------|------------------|
| **S1** | Situationâ€¯Understanding | *â€œDoes PortÂ X fall inside the uncertainty cone atÂ Tâ€‘24â€¯h?â€* |
| **S2** | Impactâ€¯Estimation | *â€œWhat is the expected recovery duration (days)?â€* |
| **S3** | Decisionâ€¯Reasoning | *â€œWhich portâ€‘condition bulletin should be issued now?â€* |

---

## Repository layout

```
Datasets/
  CyPortQA/                 # benchmark JSON + templates
  Source_data/              # raw NOAA / USCG / experiment data (â‡¢Â GitÂ LFS)
Codes/
  DataÂ Collection/          # scripts that build Source_data/
  SenarioÂ Encoding/         # converts raw data â†’ Encoded_senario.JSON
  ModelÂ RuningÂ -Â Colab/     # Colab notebooks for baseline runs
  ModelÂ RuningÂ -Â Local/     # local inference scripts (PythonÂ â‰¥Â 3.10)
  o3Â LLMÂ Judger/            # evaluation prompts + judge harness
  PerformanceÂ Eval/         # aggregation & plotting utilities
requirements.txt            # pip alternative to the condaÂ env below
environment.yml             # full Anaconda environment (recommended)
LICENSE                     # MIT
README.md                   # you are here
```

> **Note**Â Â Large files (>Â 100â€¯MB) are tracked with **GitÂ LFS** to keep the repo lightweight.

---

## Quick start

### 1Â Â·Â Set up the environment (conda recommended)

```bash
# clone anonymously (no forks that reveal identity)
git clone https://github.com/anon-researcher/CyPortQA-Anon.git
cd CyPortQA-Anon

# create & activate environment
conda env create -f environment.yml
conda activate cyportqa
# â€‘ or â€‘
# pip install -r requirements.txt
```

<details>
<summary>ðŸ“¦Â <code>environment.yml</code> (click to expand)</summary>

```yaml
name: cyportqa
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.10
  - pip
  - git-lfs
  - pip:
      - torch>=2.2
      - transformers>=4.43
      - datasets>=2.19
      - tiktoken>=0.6
      - numpy
      - pandas
      - matplotlib
      - tqdm
      - scikit-learn
```

</details>

### 2Â Â·Â Run a baseline (Colab or local)
Open the notebook in **`Codes/ModelÂ RuningÂ -Â Colab/`** *or* the script in **`ModelÂ RuningÂ -Â Local/`** and supply your API key(s) when prompted:

```bash
export OPENAI_API_KEY=xxxxx   # ChatGPTâ€‘4o
export GEMINI_API_KEY=xxxxx   # GeminiÂ 2.5 Flashâ€‘Lite
```

The runner downloads `CyPortQA.JSON`, performs inference, and writes model outputs to `Datasets/Source_data/Experiments_data/<model_name>/`.

### 3Â Â·Â Judge responses

```bash
python Codes/o3\ LLM\ Judger/judge.py \
       --pred_dir Datasets/Source_data/Experiments_data/<model_name>/ \
       --save_path Datasets/Source_data/Experiments_data/<model_name>_scored.jsonl
```

### 4Â Â·Â Aggregate scores & plot

```bash
python Codes/Performance\ Eval/aggregate.py --root Datasets/Source_data/Experiments_data/
```

---

## Dataset schema

* `CyPortQA.JSON`Â Â Â Â Â Â Â Â Â Â Â Â Â Â root list of QA items
* `CyPortQA_template.JSON`Â Â Â Â  template library (48Â templates)
* `Encoded_senario.JSON`Â Â Â Â Â Â  encoded scenario metadata (weatherÂ +Â port info)

Each QA record contains:
```jsonc
{
  "qa_id": "S1.1-000042",
  "scenario_id": "HARVEY_2017_PORT001_Tm24h",
  "task": "S1.1",           // task category
  "question": "Does PortÂ X â€¦?",
  "answer": "True",         // groundâ€‘truth
  "metadata": { ... }        // cone geoJSON, forecast table slice, etc.
}
```

> The benchmark *does not* redistribute raw NOAA imagery; scripts in `DataÂ Collection/` download them from the official public endpoints given a scenario timestamp.

---

## Adding new models

1. Place your inference script or notebook under `Codes/ModelÂ RuningÂ -*`.
2. Save predictions to `Datasets/Source_data/Experiments_data/<model_name>/` (one JSONL per scenario).
3. Reâ€‘run the judging and aggregation steps above.

---

## .gitignore (excerpt)
```
# bookkeeping
__pycache__/
*.log
*.ipynb_checkpoints/

# credentials
.env
*.key

# large or generated artefacts
Datasets/Source_data/Experiments_data/
Datasets/Source_data/NOAA*/
Datasets/Source_data/CyPort*Events/
```

---

## License

CyPortQA is released under the **MIT License**. See the [LICENSE](LICENSE) file for the full text.

---

## Citation (to be updated postâ€‘review)

```bibtex
@misc{cyportqa2025,
  title  = {CyPortQA: Benchmarking Multimodal LLMs for Cyclone Preparedness in Port Operation},
  year   = {2025},
  note   = {Anonymous submission, AAAIÂ 2026}
}
```
